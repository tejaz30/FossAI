# ğŸ§  Residual Learning with ResNet-18 on CIFAR-10

This project demonstrates the training and evaluation of a **ResNet-18** model on the **CIFAR-10** dataset using PyTorch, focusing on **residual learning**, **model improvement through hyperparameter tuning**, and **training visualization**. It includes multiple experiments using learning rate scheduling, weight decay, and K-Fold Cross Validation.

---

## ğŸ“Œ Project Overview

### âœ… What is Residual Learning?

Traditional deep networks often suffer from the **vanishing gradient problem**, making it hard to train deeper models effectively. **Residual learning**, introduced in the [ResNet paper (He et al., 2015)](https://arxiv.org/abs/1512.03385), solves this by introducing **skip connections**, allowing the network to learn residuals instead of full transformations. This increases the relevance of the data presented to the deeper layers of the network aswell as the gradient value presented to each weight and by extension to the changes to the neuron. 

**Without residuals:**
> H(x) = desired mapping

**With residuals (ResNet):**
> H(x) = F(x) + x  
> â†’ The network learns the residual: F(x) = H(x) - x

This architecture allows training of very deep networks (50, 101, 152+ layers) without degradation.

---

### ğŸ§ª Experiments Conducted

| Experiment              | Optimizer | Weight Decay | LR Scheduler | Accuracy â†‘ | Notes                               |
|-------------------------|-----------|--------------|--------------|------------|-------------------------------------|
| `exp1_no_weight_decay`  | SGD       | âŒ           | âŒ           | ~58%       | Baseline                            |
| `exp2_with_wd_scheduler`| Adam      | âœ…           | StepLR       | ~65%       | Improved generalization             |
| `exp3_kfold`            | Adam      | âœ…           | StepLR       | ~75% (avg across folds) | Better model stability |
| 'exp4_random_search`    | Adam      | âœ…           | StepLR       | ~78%       | Hyperparameter Tuning              |

ğŸ“Š Training accuracy and loss curves are saved after each run in `/trials/1_try`.

ğŸ“‚ Checkpoints for trained models are saved in `/checkpoints`.

---

## ğŸ”§ Project Structure
<Pre>

RESNET_CIFAR10/
â”‚
â”œâ”€â”€ checkpoints/                  # Final trained model weights
â”œâ”€â”€ data/                         # CIFAR-10 dataset storage
â”œâ”€â”€ graphs/                       # (Optional) Graph assets for reports
â”œâ”€â”€ Intermediate_models/          # Models & configs saved during training
â”‚   â””â”€â”€ trial-1/
â”‚       â”œâ”€â”€ config.yaml
â”‚       â”œâ”€â”€ train.py
â”‚       â””â”€â”€ kFold_train_18.py
â”‚
â”œâ”€â”€ models/                       # Finalized or best models
â”œâ”€â”€ results/                      # Metrics, logs, or analysis outputs
â”œâ”€â”€ trials/                       # Organized experiment runs
â”‚   â”œâ”€â”€ 1_try/
â”‚   â”‚   â”œâ”€â”€ trial_1/
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ trial_5/
â”‚   â””â”€â”€ 2_try/
â”‚
â”œâ”€â”€ inference.py                  # Model inference on test images
â”œâ”€â”€ kFold_train_20.py             # K-Fold cross-validation training script
â”œâ”€â”€ random_search_train.py        # Hyperparameter tuning using random search
â”œâ”€â”€ resnet.py                     # Custom ResNet wrapper if used
â”œâ”€â”€ best_config.yaml              # Best config found after tuning
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                    # Files/folders excluded from Git
â””â”€â”€ README.md                     # Project documentation
</Pre>



---

## âš™ï¸ Setup Instructions

### 1. Clone the Repo
```bash
git clone https://github.com/yourusername/resnet-cifar10.git
cd resnet-cifar10
```
### 2. Create and Activate Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
```

### 3. Install Requirements
```bash
pip install -r requirements.txt
```
## Training the Model
To start training with current config:

```bash
python train.py
```
You can change the training configuration (batch size, learning rate, etc.) via config.yaml.

---

## Hyperparameter Tuning + K-Fold Cross Validation
Implemented features:

 Optimizer comparison (SGD vs Adam)

 Weight Decay (L2 regularization)

 Learning Rate Scheduling (StepLR)

 K-Fold Cross Validation (e.g., K=5) with average performance tracking

 ---

 ## Learning and Objectives

Residual learning helps deeper models converge faster and generalize better as the learning process for a particular neuron is made easier.

Weight decay reduced overfitting and improved test accuracy.

StepLR scheduler smoothed training and helped escape local minima.

K-Fold cross-validation gave more reliable performance evaluation.

Implementing Cross-validation further removes the problem of overfitting aswell as really generalizes the model over all combinations of data.

Deeper Networks would perform better but the most important is Hyperprarameter tuning.

Experiment tracking and visualization made model behavior transparent and reproducible.

---

## Author
Teja Bulusu
Learning Deep Neural Networks â€“ 2025


